{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20696,
     "status": "ok",
     "timestamp": 1702953343059,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "jDnrNw8paoV-",
    "outputId": "e1ea86de-3aff-4d5e-bc52-44690751633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7317,
     "status": "ok",
     "timestamp": 1702953427555,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "NkUF1u-Nbahy",
    "outputId": "41062f87-9e6b-458d-c1e3-68ecc261048b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19983,
     "status": "ok",
     "timestamp": 1702953447533,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "MpPQvHwxqfcm",
    "outputId": "e30ed8c0-876e-4324-f923-8be8f7763050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/647.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m645.1/647.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=552450 sha256=35051f11a413e9937ab0a374b752afe75d211d0accb08e1bd2ad7873cd59a3be\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHcOx0KpbevL"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from annoy import AnnoyIndex\n",
    "import polars as pl\n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "import scipy.sparse as sp\n",
    "from scipy import linalg\n",
    "from scipy.special import iv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn6Pwz_eMmIZ"
   },
   "outputs": [],
   "source": [
    "LOCALES = [\"UK\", \"JP\", \"DE\"]\n",
    "VER = \"03\"\n",
    "DIR = \"/content/drive/MyDrive/kddcup2023/\"\n",
    "TOP_N = 25\n",
    "\n",
    "# hyper-parameter for proNE\n",
    "EMB_DIM = 1024\n",
    "N_EPOCH = 10\n",
    "MU = 0.0\n",
    "THETA = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMxAop-9qlkl"
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOowpQn04wvP"
   },
   "outputs": [],
   "source": [
    "def preprocess(df:pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.explode([\"prev_items\"])\n",
    "    df = df.with_columns(\n",
    "        df.select(pl.col(\"session_id\").cumcount().over(\"session_id\").alias(\"sequence_num\"))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKX8sPga64Pw"
   },
   "outputs": [],
   "source": [
    "def build_graph(df:pl.DataFrame):\n",
    "    df = df.sort([\"session_id\", \"sequence_num\"], descending=[False, False])\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"item\").shift().over(\"session_id\").alias(\"prev_item\")\n",
    "    )\n",
    "    df = df.filter(\n",
    "        (pl.col(\"prev_item\").is_not_null()) &\n",
    "        (pl.col(\"prev_item\") != pl.col(\"item\"))\n",
    "    )\n",
    "    df = df[[\"item\", \"prev_item\"]]\n",
    "    # df = df.unique()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wt0qugkNuVS"
   },
   "outputs": [],
   "source": [
    "def convert_item_id(df:pl.DataFrame):\n",
    "    unique_item_ids = sorted(list(set(df[\"item\"].unique().to_list() + df[\"prev_item\"].unique().to_list())))\n",
    "    item_id2index = dict(zip(unique_item_ids, range(len(unique_item_ids))))\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"item\").map_dict(item_id2index).alias(\"item\"),\n",
    "        pl.col(\"prev_item\").map_dict(item_id2index).alias(\"prev_item\"),\n",
    "    ])\n",
    "    return df, unique_item_ids, item_id2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "depfH7Wbh-3E"
   },
   "outputs": [],
   "source": [
    "# https://github.com/THUDM/ProNE/blob/master/proNE.py\n",
    "class ProNE():\n",
    "\tdef __init__(self, graph_file, emb_file1, emb_file2, dimension):\n",
    "\t\tself.graph = graph_file\n",
    "\t\tself.emb1 = emb_file1\n",
    "\t\tself.emb2 = emb_file2\n",
    "\t\tself.dimension = dimension\n",
    "\n",
    "\t\tself.G = nx.read_edgelist(self.graph, nodetype=int, create_using=nx.DiGraph())\n",
    "\t\tself.G = self.G.to_undirected()\n",
    "\t\tself.node_number = self.G.number_of_nodes()\n",
    "\t\tmatrix0 = scipy.sparse.lil_matrix((self.node_number, self.node_number))\n",
    "\n",
    "\t\tfor e in self.G.edges():\n",
    "\t\t\tif e[0] != e[1]:\n",
    "\t\t\t\tmatrix0[e[0], e[1]] = 1\n",
    "\t\t\t\tmatrix0[e[1], e[0]] = 1\n",
    "\t\tself.matrix0 = scipy.sparse.csr_matrix(matrix0)\n",
    "\t\tprint(matrix0.shape)\n",
    "\n",
    "\tdef get_embedding_rand(self, matrix):\n",
    "\t\t# Sparse randomized tSVD for fast embedding\n",
    "\t\tt1 = time.time()\n",
    "\t\tl = matrix.shape[0]\n",
    "\t\tsmat = scipy.sparse.csc_matrix(matrix)  # convert to sparse CSC format\n",
    "\t\tprint('svd sparse', smat.data.shape[0] * 1.0 / l ** 2)\n",
    "\t\tU, Sigma, VT = randomized_svd(smat, n_components=self.dimension, n_iter=5, random_state=None)\n",
    "\t\tU = U * np.sqrt(Sigma)\n",
    "\t\tU = preprocessing.normalize(U, \"l2\")\n",
    "\t\tprint('sparsesvd time', time.time() - t1)\n",
    "\t\treturn U\n",
    "\n",
    "\tdef get_embedding_dense(self, matrix, dimension):\n",
    "\t\t# get dense embedding via SVD\n",
    "\t\tt1 = time.time()\n",
    "\t\tU, s, Vh = linalg.svd(matrix, full_matrices=False, check_finite=False, overwrite_a=True)\n",
    "\t\tU = np.array(U)\n",
    "\t\tU = U[:, :dimension]\n",
    "\t\ts = s[:dimension]\n",
    "\t\ts = np.sqrt(s)\n",
    "\t\tU = U * s\n",
    "\t\tU = preprocessing.normalize(U, \"l2\")\n",
    "\t\tprint('densesvd time', time.time() - t1)\n",
    "\t\treturn U\n",
    "\n",
    "\tdef pre_factorization(self, tran, mask):\n",
    "\t\t# Network Embedding as Sparse Matrix Factorization\n",
    "\t\tt1 = time.time()\n",
    "\t\tl1 = 0.75\n",
    "\t\tC1 = preprocessing.normalize(tran, \"l1\")\n",
    "\t\tneg = np.array(C1.sum(axis=0))[0] ** l1\n",
    "\n",
    "\t\tneg = neg / neg.sum()\n",
    "\n",
    "\t\tneg = scipy.sparse.diags(neg, format=\"csr\")\n",
    "\t\tneg = mask.dot(neg)\n",
    "\t\tprint(\"neg\", time.time() - t1)\n",
    "\n",
    "\t\tC1.data[C1.data <= 0] = 1\n",
    "\t\tneg.data[neg.data <= 0] = 1\n",
    "\n",
    "\t\tC1.data = np.log(C1.data)\n",
    "\t\tneg.data = np.log(neg.data)\n",
    "\n",
    "\t\tC1 -= neg\n",
    "\t\tF = C1\n",
    "\t\tfeatures_matrix = self.get_embedding_rand(F)\n",
    "\t\treturn features_matrix\n",
    "\n",
    "\tdef chebyshev_gaussian(self, A, a, order=10, mu=0.5, s=0.5):\n",
    "\t\t# NE Enhancement via Spectral Propagation\n",
    "\t\tprint('Chebyshev Series -----------------')\n",
    "\t\tt1 = time.time()\n",
    "\n",
    "\t\tif order == 1:\n",
    "\t\t\treturn a\n",
    "\n",
    "\t\tA = sp.eye(self.node_number) + A\n",
    "\t\tDA = preprocessing.normalize(A, norm='l1')\n",
    "\t\tL = sp.eye(self.node_number) - DA\n",
    "\n",
    "\t\tM = L - mu * sp.eye(self.node_number)\n",
    "\n",
    "\t\tLx0 = a\n",
    "\t\tLx1 = M.dot(a)\n",
    "\t\tLx1 = 0.5 * M.dot(Lx1) - a\n",
    "\n",
    "\t\tconv = iv(0, s) * Lx0\n",
    "\t\tconv -= 2 * iv(1, s) * Lx1\n",
    "\t\tfor i in range(2, order):\n",
    "\t\t\tLx2 = M.dot(Lx1)\n",
    "\t\t\tLx2 = (M.dot(Lx2) - 2 * Lx1) - Lx0\n",
    "\t\t\t#         Lx2 = 2*L.dot(Lx1) - Lx0\n",
    "\t\t\tif i % 2 == 0:\n",
    "\t\t\t\tconv += 2 * iv(i, s) * Lx2\n",
    "\t\t\telse:\n",
    "\t\t\t\tconv -= 2 * iv(i, s) * Lx2\n",
    "\t\t\tLx0 = Lx1\n",
    "\t\t\tLx1 = Lx2\n",
    "\t\t\tdel Lx2\n",
    "\t\t\tprint('Bessell time', i, time.time() - t1)\n",
    "\t\tmm = A.dot(a - conv)\n",
    "\t\temb = self.get_embedding_dense(mm, self.dimension)\n",
    "\t\treturn emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YN2ceAx0iAo0"
   },
   "outputs": [],
   "source": [
    "# https://github.com/THUDM/ProNE/blob/master/proNE.py\n",
    "def save_embedding(emb_file, features):\n",
    "\t# save node embedding into emb_file with word2vec format\n",
    "\tf_emb = open(emb_file, 'w')\n",
    "\tf_emb.write(str(len(features)) + \" \" + str(features.shape[1]) + \"\\n\")\n",
    "\tfor i in range(len(features)):\n",
    "\t\ts = str(i) + \" \" + \" \".join(str(f) for f in features[i].tolist())\n",
    "\t\tf_emb.write(s + \"\\n\")\n",
    "\tf_emb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbMnkmd_quPq"
   },
   "outputs": [],
   "source": [
    "def generate_annoy_index(matrix):\n",
    "    index = AnnoyIndex(EMB_DIM, 'angular')\n",
    "\n",
    "    for idx,idx_embedding in enumerate(matrix):\n",
    "        index.add_item(idx, idx_embedding)\n",
    "\n",
    "    index.build(50)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSFF1ntZrYpS"
   },
   "outputs": [],
   "source": [
    "def make_nns_matrix(annoy_index, item_ids, item_id2index, k=100):\n",
    "    aid_xs = []\n",
    "    aid_ys = []\n",
    "    dists = []\n",
    "\n",
    "    for item_id in tqdm(item_ids):\n",
    "        item_index = item_id2index[item_id]\n",
    "        nns = annoy_index.get_nns_by_item(item_index, k+1, include_distances=True)\n",
    "        aid_y = [item_ids[idx] for idx in list(nns[0][1:])]\n",
    "        dist = list(nns[1][1:])\n",
    "        aid_xs.extend([item_id] * k)\n",
    "        aid_ys.extend(aid_y)\n",
    "        dists.extend(dist)\n",
    "    df = pl.DataFrame({\"item\": aid_xs, 'candidate_item': aid_ys, 'prone_distance': dists})\n",
    "\n",
    "    # rank付与\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"prone_distance\").rank(descending=False, method=\"min\").over(\"item\").alias(\"prone_rank\")\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu0fkMJULkfM"
   },
   "source": [
    "# for local train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjmWSU80Ulj-"
   },
   "outputs": [],
   "source": [
    "train = pl.read_parquet(DIR + \"data/preprocessed/task1/train_task1.parquet\")\n",
    "test1_1 = pl.read_parquet(DIR + \"data/preprocessed/task1/test_task1_phase1.parquet\")\n",
    "test1_2 = pl.read_parquet(DIR + \"data/preprocessed/task1/test_task1_phase2.parquet\")\n",
    "test2_1 = pl.read_parquet(DIR + \"data/preprocessed/task2/test_task2_phase1.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test2_2 = pl.read_parquet(DIR + \"data/preprocessed/task2/test_task2_phase2.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test3_1 = pl.read_parquet(DIR + \"data/preprocessed/task3/test_task3_phase1.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test3_2 = pl.read_parquet(DIR + \"data/preprocessed/task3/test_task3_phase2.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test1_1 = test1_1.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task1\").alias(\"session_id\")\n",
    ")\n",
    "test1_2 = test1_2.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task1\").alias(\"session_id\")\n",
    ")\n",
    "test3_1 = test3_1.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task3\").alias(\"session_id\")\n",
    ")\n",
    "test3_2 = test3_2.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task3\").alias(\"session_id\")\n",
    ")\n",
    "test = pl.concat([test1_1, test1_2, test2_1, test2_2, test3_1, test3_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrGYB8J04r3j"
   },
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROxVlwKPO2WP"
   },
   "outputs": [],
   "source": [
    "df = pl.concat([\n",
    "    train[\"prev_items\", \"locale\", \"session_id\", \"sequence_num\"],\n",
    "    test[\"prev_items\", \"locale\", \"session_id\", \"sequence_num\"],\n",
    "])\n",
    "df = df.rename({\"prev_items\":\"item\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5503492,
     "status": "ok",
     "timestamp": 1702958977772,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "GTzM5_dJmmKg",
    "outputId": "c88bf2d1-d06c-4ca7-9e4a-967ddd45912e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473418, 473418)\n",
      "neg 1.2119967937469482\n",
      "svd sparse 1.791839874422657e-05\n",
      "sparsesvd time 733.0011887550354\n",
      "Chebyshev Series -----------------\n",
      "Bessell time 2 57.82320499420166\n",
      "Bessell time 3 87.15208148956299\n",
      "Bessell time 4 116.2196991443634\n",
      "Bessell time 5 145.5481686592102\n",
      "Bessell time 6 173.7848174571991\n",
      "Bessell time 7 202.98440098762512\n",
      "Bessell time 8 232.3541181087494\n",
      "Bessell time 9 261.7687921524048\n",
      "densesvd time 99.08219075202942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 473418/473418 [10:55<00:00, 722.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(373807, 373807)\n",
      "neg 1.5809979438781738\n",
      "svd sparse 2.3035149620403156e-05\n",
      "sparsesvd time 576.9635345935822\n",
      "Chebyshev Series -----------------\n",
      "Bessell time 2 55.04509615898132\n",
      "Bessell time 3 79.03072357177734\n",
      "Bessell time 4 100.2528784275055\n",
      "Bessell time 5 121.26652598381042\n",
      "Bessell time 6 141.76309394836426\n",
      "Bessell time 7 162.30253314971924\n",
      "Bessell time 8 182.94527626037598\n",
      "Bessell time 9 202.8392882347107\n",
      "densesvd time 78.34906077384949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373807/373807 [08:35<00:00, 725.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(487500, 487500)\n",
      "neg 0.4315495491027832\n",
      "svd sparse 1.6712718474687705e-05\n",
      "sparsesvd time 733.8501851558685\n",
      "Chebyshev Series -----------------\n",
      "Bessell time 2 53.11183285713196\n",
      "Bessell time 3 78.73552179336548\n",
      "Bessell time 4 103.45490384101868\n",
      "Bessell time 5 136.52362060546875\n",
      "Bessell time 6 181.7459499835968\n",
      "Bessell time 7 209.84808897972107\n",
      "Bessell time 8 237.5414674282074\n",
      "Bessell time 9 263.5904619693756\n",
      "densesvd time 99.98243403434753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 487500/487500 [10:12<00:00, 796.29it/s]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for locale in LOCALES:\n",
    "    # filter by loacle\n",
    "    df_by_locale = df.filter(pl.col(\"locale\") == locale)\n",
    "\n",
    "    # build graph\n",
    "    graph_df = build_graph(df_by_locale)\n",
    "    graph_df, item_ids, item_id2index = convert_item_id(graph_df)\n",
    "    graph_df.write_csv(DIR + f\"data/interim/graph/task1/edge_list_{VER}_{locale}_task1_for_local_train_or_eval.txt\", has_header=False, separator=\" \")\n",
    "    item_ids_name = f\"item_id2index_{VER}_{locale}_for_train_or_eval.pickle\"\n",
    "    with open(DIR + \"data/interim/graph/task1/graph_\" + item_ids_name, \"wb\") as f:\n",
    "        pickle.dump(item_id2index, f)\n",
    "\n",
    "    # generate graph embedding by proNE\n",
    "    model = ProNE(\n",
    "        DIR + f\"data/interim/graph/task1/edge_list_{VER}_{locale}_task1_for_local_train_or_eval.txt\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        EMB_DIM,\n",
    "    )\n",
    "    features_matrix = model.pre_factorization(model.matrix0, model.matrix0)\n",
    "    embeddings_matrix = model.chebyshev_gaussian(model.matrix0, features_matrix, N_EPOCH, MU, THETA)\n",
    "    np.save(DIR + f\"models/task1/graph_embedding_{VER}_{locale}_for_local_train_or_eval.npy\", embeddings_matrix)\n",
    "\n",
    "    # generate_candidates\n",
    "    annoy_index = generate_annoy_index(embeddings_matrix)\n",
    "    candidate = make_nns_matrix(annoy_index, item_ids, item_id2index, k=TOP_N)\n",
    "    candidate = candidate.with_columns(pl.lit(locale).alias(\"locale\"))\n",
    "    candidates.append(candidate)\n",
    "\n",
    "candidate = pl.concat(candidates)\n",
    "candidate.write_parquet(DIR + f\"data/interim/candidates/task1/prone_{VER}_for_local_or_eval.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1702958977776,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "3CJMDXJxxCp7",
    "outputId": "1def9c8d-6b44-4997-ca25-e4cd57389c90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>item</th><th>candidate_item</th><th>prone_distance</th><th>prone_rank</th><th>locale</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>u32</td><td>str</td></tr></thead><tbody><tr><td>&quot;0001821946&quot;</td><td>&quot;0008326045&quot;</td><td>0.30018</td><td>1</td><td>&quot;UK&quot;</td></tr><tr><td>&quot;0001821946&quot;</td><td>&quot;0007215991&quot;</td><td>0.592588</td><td>2</td><td>&quot;UK&quot;</td></tr><tr><td>&quot;0001821946&quot;</td><td>&quot;0007892314&quot;</td><td>0.599894</td><td>3</td><td>&quot;UK&quot;</td></tr><tr><td>&quot;0001821946&quot;</td><td>&quot;1408367289&quot;</td><td>0.638336</td><td>4</td><td>&quot;UK&quot;</td></tr><tr><td>&quot;0001821946&quot;</td><td>&quot;0241348919&quot;</td><td>0.63888</td><td>5</td><td>&quot;UK&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌────────────┬────────────────┬────────────────┬────────────┬────────┐\n",
       "│ item       ┆ candidate_item ┆ prone_distance ┆ prone_rank ┆ locale │\n",
       "│ ---        ┆ ---            ┆ ---            ┆ ---        ┆ ---    │\n",
       "│ str        ┆ str            ┆ f64            ┆ u32        ┆ str    │\n",
       "╞════════════╪════════════════╪════════════════╪════════════╪════════╡\n",
       "│ 0001821946 ┆ 0008326045     ┆ 0.30018        ┆ 1          ┆ UK     │\n",
       "│ 0001821946 ┆ 0007215991     ┆ 0.592588       ┆ 2          ┆ UK     │\n",
       "│ 0001821946 ┆ 0007892314     ┆ 0.599894       ┆ 3          ┆ UK     │\n",
       "│ 0001821946 ┆ 1408367289     ┆ 0.638336       ┆ 4          ┆ UK     │\n",
       "│ 0001821946 ┆ 0241348919     ┆ 0.63888        ┆ 5          ┆ UK     │\n",
       "└────────────┴────────────────┴────────────────┴────────────┴────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtnVLCFqsTc5"
   },
   "source": [
    "# MRR@100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSIjE0HtnWC_"
   },
   "outputs": [],
   "source": [
    "train = pl.read_parquet(DIR + \"data/preprocessed/task1/train_task1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G1BzR1Buy-Q"
   },
   "outputs": [],
   "source": [
    "candidate = pl.read_parquet(DIR + f\"data/interim/candidates/task1/prone_{VER}_for_local_or_eval.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpiLk_urnWFH"
   },
   "outputs": [],
   "source": [
    "# last_item\n",
    "last_item_list = []\n",
    "prev_items_list = train[\"prev_items\"].to_list()\n",
    "for prev_items in prev_items_list:https://claude.ai/chats\n",
    "    last_item_list.append(prev_items[-1])\n",
    "train = train.with_columns(pl.Series(name=\"last_item\", values=last_item_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Mx8wqWlnWK1"
   },
   "outputs": [],
   "source": [
    "label_lists = []\n",
    "for locale in LOCALES:\n",
    "    df = train.filter(pl.col(\"locale\") == locale)\n",
    "    df = df.join(candidate, left_on=[\"locale\", \"last_item\"], right_on=[\"locale\", \"item\"], how=\"left\")\n",
    "    df = df.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\")))\n",
    "    df = df.sort([\"session_id\", \"prone_distance\"], descending=[False, False])\n",
    "    df = df.with_columns((pl.col(\"candidate_item\") == pl.col(\"next_item\")).cast(pl.Int8).alias(\"label\"))\n",
    "    label_lists.extend(df.groupby(\"session_id\", maintain_order=True).all()[\"label\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8187,
     "status": "ok",
     "timestamp": 1702959138103,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "K5TXGiKZpth0",
    "outputId": "30876133-a607-4a2d-ed13-1789426e28a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.11598\n"
     ]
    }
   ],
   "source": [
    "# MRR\n",
    "rr = 0\n",
    "for labels in label_lists:\n",
    "    labels = labels[:100]\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            rr += 1 / (i+1)\n",
    "            break\n",
    "mrr = rr / len(label_lists)\n",
    "print(\"MRR:\", round(mrr, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "339imMwzxGhm"
   },
   "source": [
    "# for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFmXpsRiuCPs"
   },
   "outputs": [],
   "source": [
    "train = pl.read_parquet(DIR + \"data/preprocessed/task1/train_task1.parquet\")\n",
    "test1_1 = pl.read_parquet(DIR + \"data/preprocessed/task1/test_task1_phase1.parquet\")\n",
    "test1_2 = pl.read_parquet(DIR + \"data/preprocessed/task1/test_task1_phase2.parquet\")\n",
    "test2_1 = pl.read_parquet(DIR + \"data/preprocessed/task2/test_task2_phase1.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test2_2 = pl.read_parquet(DIR + \"data/preprocessed/task2/test_task2_phase2.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test3_1 = pl.read_parquet(DIR + \"data/preprocessed/task3/test_task3_phase1.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test3_2 = pl.read_parquet(DIR + \"data/preprocessed/task3/test_task3_phase2.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test1_1 = test1_1.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task1\").alias(\"session_id\")\n",
    ")\n",
    "test1_2 = test1_2.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task1\").alias(\"session_id\")\n",
    ")\n",
    "test3_1 = test3_1.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task3\").alias(\"session_id\")\n",
    ")\n",
    "test3_2 = test3_2.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task3\").alias(\"session_id\")\n",
    ")\n",
    "test = pl.concat([test1_1, test1_2, test2_1, test2_2, test3_1, test3_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwnu_xPGxfeo"
   },
   "outputs": [],
   "source": [
    "# Append train's next_item to prev_items\n",
    "prev_items_list = train[\"prev_items\"].to_list()\n",
    "next_item_list = train[\"next_item\"].to_list()\n",
    "prev_items_list_updated = []\n",
    "for prev_items, next_item in zip(prev_items_list, next_item_list):\n",
    "    prev_items.append(next_item)\n",
    "    prev_items_list_updated.append(prev_items)\n",
    "\n",
    "train = train.with_columns(\n",
    "    pl.Series(name=\"prev_items\", values=prev_items_list_updated)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awkaL22fxfhV"
   },
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "df = pl.concat([\n",
    "    train[\"prev_items\", \"locale\", \"session_id\", \"sequence_num\"],\n",
    "    test[\"prev_items\", \"locale\", \"session_id\", \"sequence_num\"],\n",
    "])\n",
    "df = df.rename({\"prev_items\":\"item\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5918090,
     "status": "ok",
     "timestamp": 1702965112213,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "XGaLYfgrxwKD",
    "outputId": "b79e22d7-52de-434e-b62f-015918dd148c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500011, 500011)\n",
      "neg 1.7195041179656982\n",
      "svd sparse 2.1335357233955394e-05\n",
      "sparsesvd time 873.7156457901001\n",
      "Chebyshev Series -----------------\n",
      "Bessell time 2 56.96180486679077\n",
      "Bessell time 3 91.36886072158813\n",
      "Bessell time 4 120.405264377594\n",
      "Bessell time 5 149.59175276756287\n",
      "Bessell time 6 178.4927749633789\n",
      "Bessell time 7 208.60589957237244\n",
      "Bessell time 8 237.99869966506958\n",
      "Bessell time 9 266.80463576316833\n",
      "densesvd time 100.29109501838684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500011/500011 [09:58<00:00, 835.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(394847, 394847)\n",
      "neg 0.6970870494842529\n",
      "svd sparse 2.680233918488267e-05\n",
      "sparsesvd time 676.5158019065857\n",
      "Chebyshev Series -----------------\n",
      "Bessell time 2 45.175753593444824\n",
      "Bessell time 3 67.61637258529663\n",
      "Bessell time 4 90.09317445755005\n",
      "Bessell time 5 112.55969667434692\n",
      "Bessell time 6 137.50688433647156\n",
      "Bessell time 7 159.94562411308289\n",
      "Bessell time 8 182.163268327713\n",
      "Bessell time 9 204.44109892845154\n",
      "densesvd time 79.68171000480652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394847/394847 [07:43<00:00, 851.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(518182, 518182)\n",
      "neg 2.4433209896087646\n",
      "svd sparse 1.9364790658131597e-05\n",
      "sparsesvd time 913.7967445850372\n",
      "Chebyshev Series -----------------\n",
      "Bessell time 2 61.45081925392151\n",
      "Bessell time 3 91.25773811340332\n",
      "Bessell time 4 121.18132972717285\n",
      "Bessell time 5 150.7693269252777\n",
      "Bessell time 6 181.00627541542053\n",
      "Bessell time 7 210.36813139915466\n",
      "Bessell time 8 240.61320805549622\n",
      "Bessell time 9 270.07284903526306\n",
      "densesvd time 112.58931398391724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518182/518182 [10:47<00:00, 799.73it/s]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for locale in LOCALES:\n",
    "    # filter by loacle\n",
    "    df_by_locale = df.filter(pl.col(\"locale\") == locale)\n",
    "\n",
    "    # build graph\n",
    "    graph_df = build_graph(df_by_locale)\n",
    "    graph_df, item_ids, item_id2index = convert_item_id(graph_df)\n",
    "    graph_df.write_csv(DIR + f\"data/interim/graph/task1/edge_list_{VER}_{locale}_task1_for_inference.txt\", has_header=False, separator=\" \")\n",
    "    item_ids_name = f\"item_id2index_{VER}_{locale}_for_inference.pickle\"\n",
    "    with open(DIR + \"data/interim/graph/task1/graph_\" + item_ids_name, \"wb\") as f:\n",
    "        pickle.dump(item_id2index, f)\n",
    "\n",
    "    # generate graph embedding by proNE\n",
    "    model = ProNE(\n",
    "        DIR + f\"data/interim/graph/task1/edge_list_{VER}_{locale}_task1_for_inference.txt\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        EMB_DIM,\n",
    "    )\n",
    "    features_matrix = model.pre_factorization(model.matrix0, model.matrix0)\n",
    "    embeddings_matrix = model.chebyshev_gaussian(model.matrix0, features_matrix, N_EPOCH, MU, THETA)\n",
    "    np.save(DIR + f\"models/task1/graph_embedding_{VER}_{locale}_for_inference.npy\", embeddings_matrix)\n",
    "\n",
    "    # generate_candidates\n",
    "    annoy_index = generate_annoy_index(embeddings_matrix)\n",
    "    candidate = make_nns_matrix(annoy_index, item_ids, item_id2index, k=TOP_N)\n",
    "    candidate = candidate.with_columns(pl.lit(locale).alias(\"locale\"))\n",
    "    candidates.append(candidate)\n",
    "\n",
    "candidate = pl.concat(candidates)\n",
    "candidate.write_parquet(DIR + f\"data/interim/candidates/task1/prone_{VER}_for_inference.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqXNT7hn2T7c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33102,
     "status": "ok",
     "timestamp": 1702913321755,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "BnKnD-UQWSEg",
    "outputId": "419dcffe-01f1-47ab-87a4-ac40941a52fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6800,
     "status": "ok",
     "timestamp": 1702913436432,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "8Af_47xxWVhC",
    "outputId": "b6672f5b-87ef-418a-db15-e2c62ef83669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28118,
     "status": "ok",
     "timestamp": 1702913464543,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "LUeEqEb6pytJ",
    "outputId": "fcd2cbac-e729-42c9-d534-5e334938b3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==4.0.1\n",
      "  Downloading gensim-4.0.1.tar.gz (23.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.11.4)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (6.4.0)\n",
      "Building wheels for collected packages: gensim\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
      "Failed to build gensim\n",
      "\u001b[31mERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install gensim==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-RBDJdwWdI-"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Union\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YamzEAnvF6Sz"
   },
   "outputs": [],
   "source": [
    "DIR = \"/content/drive/MyDrive/kddcup2023/\"\n",
    "TOP_N = 25\n",
    "LOCALES = [\"DE\", \"JP\", \"UK\"]\n",
    "VER = \"05\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTqrjJF11A4n"
   },
   "outputs": [],
   "source": [
    "def preprocess(df:pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.explode([\"prev_items\"])\n",
    "    df = df.with_columns(\n",
    "        df.select(pl.col(\"session_id\").cumcount().over(\"session_id\").alias(\"sequence_num\"))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-55dFNZvS1j"
   },
   "outputs": [],
   "source": [
    "def train_word2vec(df:pl.DataFrame) -> Word2Vec:\n",
    "\n",
    "    # Create a sequence of aids for each session\n",
    "    aid_sequences = list(df.groupby(\"session_id\", maintain_order=True).all()[\"prev_items\"].to_list())\n",
    "\n",
    "    # word2vec\n",
    "    model = Word2Vec(\n",
    "        sentences=aid_sequences,\n",
    "        epochs=25,\n",
    "        min_count=1,\n",
    "        workers=8,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjmbOhN4vE5D"
   },
   "outputs": [],
   "source": [
    "def make_nns_matrix(w2v, k):\n",
    "    aid_xs = []\n",
    "    aid_ys = []\n",
    "    sims = []\n",
    "    for aid in tqdm(w2v.wv.index_to_key):\n",
    "        nns = w2v.wv.most_similar(aid, topn=k)\n",
    "        aid_y = [x[0] for x in nns]\n",
    "        sim = [x[1] for x in nns]\n",
    "        aid_xs.extend([aid] * k)\n",
    "        aid_ys.extend(aid_y)\n",
    "        sims.extend(sim)\n",
    "\n",
    "    return pl.DataFrame({\"item\": aid_xs, \"candidate_item\": aid_ys, 'nns_similality': sims})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WePFxwftxF2S"
   },
   "source": [
    "# For test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glff-u5Y7vCm"
   },
   "outputs": [],
   "source": [
    "train = pl.read_parquet(DIR + \"data/preprocessed/task1/train_task1.parquet\")\n",
    "test1_1 = pl.read_parquet(DIR + \"data/preprocessed/task1/test_task1_phase1.parquet\")\n",
    "test1_2 = pl.read_parquet(DIR + \"data/preprocessed/task1/test_task1_phase2.parquet\")\n",
    "test2_1 = pl.read_parquet(DIR + \"data/preprocessed/task2/test_task2_phase1.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test2_2 = pl.read_parquet(DIR + \"data/preprocessed/task2/test_task2_phase2.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test3_1 = pl.read_parquet(DIR + \"data/preprocessed/task3/test_task3_phase1.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test3_2 = pl.read_parquet(DIR + \"data/preprocessed/task3/test_task3_phase2.parquet\").filter( pl.col(\"locale\").is_in(LOCALES))\n",
    "test1_1 = test1_1.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task1\").alias(\"session_id\")\n",
    ")\n",
    "test1_2 = test1_2.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task1\").alias(\"session_id\")\n",
    ")\n",
    "test3_1 = test3_1.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task3\").alias(\"session_id\")\n",
    ")\n",
    "test3_2 = test3_2.with_columns(\n",
    "    (pl.col(\"session_id\") + \"_from_task3\").alias(\"session_id\")\n",
    ")\n",
    "test = pl.concat([test1_1, test1_2, test2_1, test2_2, test3_1, test3_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um9Y8XD09W2v"
   },
   "outputs": [],
   "source": [
    "# Append train's next_item to prev_items\n",
    "prev_items_list = train[\"prev_items\"].to_list()\n",
    "next_item_list = train[\"next_item\"].to_list()\n",
    "prev_items_list_updated = []\n",
    "for prev_items, next_item in zip(prev_items_list, next_item_list):\n",
    "    prev_items.append(next_item)\n",
    "    prev_items_list_updated.append(prev_items)\n",
    "\n",
    "train = train.with_columns(\n",
    "    pl.Series(name=\"prev_items\", values=prev_items_list_updated)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhFPqBM17vE5"
   },
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "session_df = pl.concat([\n",
    "    train[\"prev_items\", \"locale\", \"session_id\", \"sequence_num\"],\n",
    "    test[\"prev_items\", \"locale\", \"session_id\", \"sequence_num\"],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bhuOxE0HS5C"
   },
   "outputs": [],
   "source": [
    "# train word2vec model\n",
    "for locale in LOCALES:\n",
    "    # filter by locale\n",
    "    df = session_df.filter(pl.col(\"locale\") == locale)\n",
    "\n",
    "    # train and save word2vec model\n",
    "    model = train_word2vec(df)\n",
    "    model.save(DIR + f\"models/task1/item2vec_{locale}_{VER}_for_inference.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20565650,
     "status": "ok",
     "timestamp": 1702934996312,
     "user": {
      "displayName": "pp rich",
      "userId": "00371241177131396451"
     },
     "user_tz": -480
    },
    "id": "eBMoGRhmChei",
    "outputId": "a3b0f171-af93-432b-fef7-4c527d63f2be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518327/518327 [2:14:32<00:00, 64.21it/s]\n",
      "100%|██████████| 395009/395009 [1:17:48<00:00, 84.62it/s]\n",
      "100%|██████████| 500180/500180 [2:09:51<00:00, 64.19it/s]\n"
     ]
    }
   ],
   "source": [
    "nns_matrices = []\n",
    "for locale in LOCALES:\n",
    "    # calculate nearest neighbors\n",
    "    w2v = Word2Vec.load(DIR + f\"models/task1/item2vec_{locale}_{VER}_for_inference.model\")\n",
    "    nns_matrix = make_nns_matrix(w2v, TOP_N)\n",
    "    nns_matrix = nns_matrix.with_columns(pl.lit(locale).alias(\"locale\"))\n",
    "    nns_matrices.append(nns_matrix)\n",
    "nns_matrix = pl.concat(nns_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T77IvUeRChhA"
   },
   "outputs": [],
   "source": [
    "file_name = f\"nns_matrix_{VER}_for_inference.parquet\"\n",
    "nns_matrix.write_parquet(\"/content/drive/MyDrive/kddcup2023/data/interim/candidates/task1/\" + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE19uwdVChjf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v--J7LUqL9Cj"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnKnD-UQWSEg"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5276,
     "status": "ok",
     "timestamp": 1705842187698,
     "user": {
      "displayName": "王晓",
      "userId": "03118875830758001304"
     },
     "user_tz": -480
    },
    "id": "8Af_47xxWVhC",
    "outputId": "75aefe27-6358-401c-b812-44855c0e6630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24870,
     "status": "ok",
     "timestamp": 1705842212563,
     "user": {
      "displayName": "王晓",
      "userId": "03118875830758001304"
     },
     "user_tz": -480
    },
    "id": "ou8Cq4Rx5zhJ",
    "outputId": "afdd99f8-b4ff-4bc0-b87d-bfbcfbd61fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==4.0.1\n",
      "  Downloading gensim-4.0.1.tar.gz (23.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.11.4)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (6.4.0)\n",
      "Building wheels for collected packages: gensim\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
      "Failed to build gensim\n",
      "\u001b[31mERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-RBDJdwWdI-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEPw4yzayxut"
   },
   "source": [
    "## constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3OCFFMJZoNy"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"exp115\"\n",
    "DIR = \"/content/drive/MyDrive/kddcup2023/\"\n",
    "K_FOLDS = 3\n",
    "SEED = 42\n",
    "LOCALES = [\"UK\", \"JP\", \"DE\"]\n",
    "MAKE_TRAIN = False\n",
    "MAKE_TEST = True\n",
    "\n",
    "# This parameter controls to which end item the candidate is tied.\n",
    "# For example, if [1,2], candidates are generated from the last item and second last item in each session.\n",
    "LAST_NS = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a48esfu9yCnO"
   },
   "outputs": [],
   "source": [
    "USE_FEATURES = [\n",
    "    # === candidate features ===\n",
    "    \"co_visit_weight_last1\", \"consective_1_weight_last1\", \"consective_3_weight_last1\", \"consective_5_weight_last1\", \"similarity_score_last1\", \"bert_distance_last1\", \"lift_last1\", \"prone_distance_last1\",\n",
    "    \"co_visit_weight_last2\", \"consective_1_weight_last2\", \"consective_3_weight_last2\", \"consective_5_weight_last2\", \"similarity_score_last2\", \"bert_distance_last2\", \"lift_last2\", \"prone_distance_last2\",\n",
    "    \"co_visit_weight_last3\", \"consective_1_weight_last3\", \"consective_3_weight_last3\", \"consective_5_weight_last3\", \"similarity_score_last3\", \"bert_distance_last3\", \"lift_last3\", \"prone_distance_last3\",\n",
    "    \"imf_score\", \"bpr_score\",\n",
    "    \"co_visit_rank_last1\", \"consective_1_rank_last1\", \"consective_3_rank_last1\", \"consective_5_rank_last1\", \"similarity_rank_last1\", \"bert_rank_last1\", \"lift_rank_last1\", \"prone_rank_last1\",\n",
    "    \"co_visit_rank_last2\", \"consective_1_rank_last2\", \"consective_3_rank_last2\", \"consective_5_rank_last2\", \"similarity_rank_last2\", \"bert_rank_last2\", \"lift_rank_last2\", \"prone_rank_last2\",\n",
    "    \"co_visit_rank_last3\", \"consective_1_rank_last3\", \"consective_3_rank_last3\", \"consective_5_rank_last3\", \"similarity_rank_last3\", \"bert_rank_last3\", \"lift_rank_last3\", \"prone_rank_last3\",\n",
    "    \"imf_rank\", \"bpr_rank\",\n",
    "    # === session features ===\n",
    "    \"S_session_length\",\n",
    "    \"S_nunique_brand\",\n",
    "    \"S_ratio_unique_brand\",\n",
    "    \"S_nunique_item\",\n",
    "    \"S_ratio_repurchase\",\n",
    "    \"S_locale\",\n",
    "    \"S_mean_price\", \"S_max_price\", \"S_min_price\", \"S_std_price\", \"S_total_amount\",\n",
    "    \"S_color_not_null_count\", \"S_size_not_null_count\", \"S_model_not_null_count\", \"S_material_not_null_count\", \"S_author_not_null_count\",\n",
    "    \"S_last_item_price\",\n",
    "    # === product features ===\n",
    "    \"P_price\",\n",
    "    \"P_purchase_count\", \"P_purchase_count_global\",\n",
    "    \"P_total_amount\",\n",
    "    \"P_brand_purchase_count\", \"P_brand_purchase_count_global\",\n",
    "    \"P_brand_mean_price\", \"P_brand_max_price\", \"P_brand_min_price\", \"P_brand_std_price\", \"P_total_brand_amount\",\n",
    "    \"P_price_diff_to_avg_brand_price\",\n",
    "    \"P_n_unique_locale\",\n",
    "    \"P_is_color_null\", \"P_is_size_null\", \"P_is_model_null\", \"P_is_material_null\", \"P_is_author_null\",\n",
    "    \"P_purchase_count_ratio_to_locale\", \"P_purchase_amount_ratio_to_locale\", \"P_purchase_count_ratio_to_brand\", \"P_purchase_amount_ratio_to_brand\",\n",
    "    # === session * product features ===\n",
    "    \"SP_price_diff_to_mean_price\", \"SP_price_diff_to_min_price\", \"SP_price_diff_to_max_price\", \"SP_price_diff_to_last_price\",\n",
    "    \"SP_brand_price_diff_to_mean_price\", \"SP_brand_price_diff_to_min_price\", \"SP_brand_price_diff_to_max_price\", \"SP_brand_price_diff_to_last_price\",\n",
    "    \"SP_same_brand_last1\", \"SP_same_brand_last2\", \"SP_same_brand_last3\",\n",
    "    \"SP_same_color_last1\", \"SP_same_color_last2\", \"SP_same_color_last3\",\n",
    "    \"SP_same_size_last1\", \"SP_same_size_last2\", \"SP_same_size_last3\",\n",
    "    \"SP_same_model_last1\", \"SP_same_model_last2\", \"SP_same_model_last3\",\n",
    "    \"SP_same_material_last1\", \"SP_same_material_last2\", \"SP_same_material_last3\",\n",
    "    \"SP_same_author_last1\", \"SP_same_author_last2\", \"SP_same_author_last3\",\n",
    "    \"SP_same_brand_sum\", \"SP_same_color_sum\", \"SP_same_size_sum\", \"SP_same_model_sum\", \"SP_same_material_sum\", \"SP_same_author_sum\",\n",
    "    # === similality features ===\n",
    "    \"imf_similarity\", \"bpr_similarity\",\n",
    "    \"graph_emb_similarity_last1\", \"graph_emb_similarity_last2\", \"graph_emb_similarity_last3\",\n",
    "    \"i2v_similarity_last1\", \"i2v_similarity_last2\", \"i2v_similarity_last3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f19D1NtjyBUk"
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrSTmUlut54k"
   },
   "outputs": [],
   "source": [
    "class CandidateMatrix:\n",
    "    def __init__(self, matrix: pl.DataFrame, feat_name: List[str], join_key: str):\n",
    "        self.matrix = matrix\n",
    "        self.feat_name = feat_name\n",
    "        self.join_key = join_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYt-dX3dt-A9"
   },
   "outputs": [],
   "source": [
    "train = pl.read_parquet(DIR + \"data/preprocessed/task1/train_task1.parquet\")\n",
    "test = pl.read_parquet(DIR + \"data/preprocessed/task1/test_task1_phase2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbUyVOQYuJZs"
   },
   "outputs": [],
   "source": [
    "session_feat = pl.read_parquet(DIR + \"data/interim/features/task1/session_feature_06.parquet\")\n",
    "product_feat_train = pl.read_parquet(DIR + \"data/interim/features/task1/product_feature_train_08.parquet\")\n",
    "product_feat_test = pl.read_parquet(DIR + \"data/interim/features/task1/product_feature_test_08.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dCzdlzat_9W"
   },
   "outputs": [],
   "source": [
    "similar_products1 = pl.read_parquet(DIR + \"data/interim/candidates/task1/similar_products_13.parquet\")\n",
    "similar_products2 = pl.read_parquet(DIR + \"data/interim/candidates/task1/similar_products_19.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_HyzlOft23G"
   },
   "outputs": [],
   "source": [
    "if MAKE_TRAIN:\n",
    "    imf_candidates_train = pl.read_parquet(DIR + \"data/interim/candidates/task1/imf_15_for_train_or_eval.parquet\")\n",
    "    bpr_candidates_train = pl.read_parquet(DIR + \"data/interim/candidates/task1/bpr_01_for_train_or_eval.parquet\")\n",
    "    co_visit_matrix_train_1 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_25_for_train_or_eval.parquet\")\n",
    "    co_visit_matrix_train_2 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_30_for_train_or_eval.parquet\")\n",
    "    co_visit_matrix_train_3 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_29_for_train_or_eval.parquet\")\n",
    "    co_visit_matrix_train_4 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_27_for_train_or_eval.parquet\")\n",
    "    co_visit_matrix_train_5 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_28_for_train_or_eval.parquet\")\n",
    "    prone_matrix_train = pl.read_parquet(DIR + \"data/interim/candidates/task1/prone_03_for_local_or_eval.parquet\")\n",
    "\n",
    "    candidate_matrices_train =[\n",
    "        CandidateMatrix(co_visit_matrix_train_1, [\"co_visit_weight\", \"co_visit_rank\"], \"item\"),\n",
    "        CandidateMatrix(co_visit_matrix_train_2, [\"lift\", \"lift_rank\"], \"item\"),\n",
    "        CandidateMatrix(co_visit_matrix_train_3, [\"consective_1_weight\", \"consective_1_rank\"], \"item\"),\n",
    "        CandidateMatrix(co_visit_matrix_train_4, [\"consective_3_weight\", \"consective_3_rank\"], \"item\"),\n",
    "        CandidateMatrix(co_visit_matrix_train_5, [\"consective_5_weight\", \"consective_5_rank\"], \"item\"),\n",
    "        CandidateMatrix(similar_products1, [\"similarity_score\", \"similarity_rank\"], \"item\"),\n",
    "        CandidateMatrix(similar_products2, [\"bert_distance\", \"bert_rank\"], \"item\"),\n",
    "        CandidateMatrix(bpr_candidates_train, [\"bpr_score\", \"bpr_rank\"], \"session\"),\n",
    "        CandidateMatrix(imf_candidates_train, [\"imf_score\", \"imf_rank\"], \"session\"),\n",
    "        CandidateMatrix(prone_matrix_train, [\"prone_distance\", \"prone_rank\"], \"item\"),\n",
    "    ]\n",
    "\n",
    "    # item2vec model\n",
    "    i2v_models_train = {}\n",
    "    for locale in LOCALES:\n",
    "        i2v_models_train[locale] = Word2Vec.load(DIR + f\"models/task1/item2vec_{locale}_05_for_train_or_eval.model\")\n",
    "\n",
    "    # imf\n",
    "    imf_model_train = {}\n",
    "    user_id2index_train = {}\n",
    "    item_id2index_train = {}\n",
    "    for locale in LOCALES:\n",
    "        imf_model_train[locale] = np.load(DIR + f\"models/task1/imf_15_{locale}_model_for_train_or_eval.npz\")\n",
    "        with open(DIR + f\"models/task1/imf_15_{locale}_user_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n",
    "            user_id2index_train[locale] = pickle.load(f)\n",
    "        with open(DIR + f\"models/task1/imf_15_{locale}_item_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n",
    "            item_id2index_train[locale] = pickle.load(f)\n",
    "\n",
    "    # bpr\n",
    "    bpr_model_train = {}\n",
    "    bpr_user_id2index_train = {}\n",
    "    bpr_item_id2index_train = {}\n",
    "    for locale in LOCALES:\n",
    "        bpr_model_train[locale] = np.load(DIR + f\"models/task1/bpr_01_{locale}_model_for_train_or_eval.npz\")\n",
    "        with open(DIR + f\"models/task1/bpr_01_{locale}_user_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n",
    "            bpr_user_id2index_train[locale] = pickle.load(f)\n",
    "        with open(DIR + f\"models/task1/bpr_01_{locale}_item_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n",
    "            bpr_user_id2index_train[locale] = pickle.load(f)\n",
    "\n",
    "    # prone\n",
    "    graph_embs_train = {}\n",
    "    item_id2indices_prone_train = {}\n",
    "    for locale in LOCALES:\n",
    "        graph_embs_train[locale] = np.load(DIR + f\"models/task1/graph_embedding_03_{locale}_for_local_train_or_eval.npy\")\n",
    "        with open(DIR + \"data/interim/graph/task1/graph_\" + f\"item_id2index_03_{locale}_for_train_or_eval.pickle\", \"rb\") as f:\n",
    "            item_id2indices_prone_train[locale] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nj76vGat25L"
   },
   "outputs": [],
   "source": [
    "\n",
    "if MAKE_TEST:\n",
    "    imf_candidates_test = pl.read_parquet(DIR + \"data/interim/candidates/task1/imf_15_for_inference.parquet\")\n",
    "    bpr_candidates_test = pl.read_parquet(DIR + \"data/interim/candidates/task1/bpr_01_for_inference.parquet\")\n",
    "    co_visit_matrix_test_1 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_25_for_inference.parquet\")\n",
    "    co_visit_matrix_test_2 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_30_for_inference.parquet\")\n",
    "    co_visit_matrix_test_3 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_29_for_inference.parquet\")\n",
    "    co_visit_matrix_test_4 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_27_for_inference.parquet\")\n",
    "    co_visit_matrix_test_5 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_28_for_inference.parquet\")\n",
    "    prone_matrix_test = pl.read_parquet(DIR + \"data/interim/candidates/task1/prone_03_for_inference.parquet\")\n",
    "\n",
    "    candidate_matrices_test =[\n",
    "        CandidateMatrix(co_visit_matrix_test_1, [\"co_visit_weight\", \"co_visit_rank\"], \"item\"),\n",
    "        CandidateMatrix(co_visit_matrix_test_2, [\"lift\", \"lift_rank\"], \"item\"),\n",
    "        CandidateMatrix(co_visit_matrix_test_3, [\"consective_1_weight\", \"consective_1_rank\"], \"item\"),\n",
    "        CandidateMatrix(co_visit_matrix_test_4, [\"consective_3_weight\", \"consective_3_rank\"], \"item\"),\n",
    "        CandidateMatrix(co_visit_matrix_test_5, [\"consective_5_weight\", \"consective_5_rank\"], \"item\"),\n",
    "        CandidateMatrix(similar_products1, [\"similarity_score\", \"similarity_rank\"], \"item\"),\n",
    "        CandidateMatrix(similar_products2, [\"bert_distance\", \"bert_rank\"], \"item\"),\n",
    "        CandidateMatrix(imf_candidates_test, [\"imf_score\", \"imf_rank\"], \"session\"),\n",
    "        CandidateMatrix(bpr_candidates_test, [\"bpr_score\", \"bpr_rank\"], \"session\"),\n",
    "        CandidateMatrix(prone_matrix_test, [\"prone_distance\", \"prone_rank\"], \"item\"),\n",
    "    ]\n",
    "\n",
    "    # item2vec\n",
    "    i2v_models_test = {}\n",
    "    for locale in LOCALES:\n",
    "        i2v_models_test[locale] = Word2Vec.load(DIR + f\"models/task1/item2vec_{locale}_05_for_inference.model\")\n",
    "\n",
    "    # imf model\n",
    "    imf_model_test = {}\n",
    "    user_id2index_test = {}\n",
    "    item_id2index_test = {}\n",
    "    for locale in LOCALES:\n",
    "        imf_model_test[locale] = np.load(DIR + f\"models/task1/imf_15_{locale}_model_for_inference.npz\")\n",
    "        with open(DIR + f\"models/task1/imf_15_{locale}_user_id2index_for_inference.pickle\", \"rb\") as f:\n",
    "            user_id2index_test[locale] = pickle.load(f)\n",
    "        with open(DIR + f\"models/task1/imf_15_{locale}_item_id2index_for_inference.pickle\", \"rb\") as f:\n",
    "            item_id2index_test[locale] = pickle.load(f)\n",
    "\n",
    "    # imf model\n",
    "    bpr_model_test = {}\n",
    "    bpr_user_id2index_test = {}\n",
    "    bpr_item_id2index_test = {}\n",
    "    for locale in LOCALES:\n",
    "        bpr_model_test[locale] = np.load(DIR + f\"models/task1/bpr_01_{locale}_model_for_inference.npz\")\n",
    "        with open(DIR + f\"models/task1/bpr_01_{locale}_user_id2index_for_inference.pickle\", \"rb\") as f:\n",
    "            bpr_user_id2index_test[locale] = pickle.load(f)\n",
    "        with open(DIR + f\"models/task1/bpr_01_{locale}_item_id2index_for_inference.pickle\", \"rb\") as f:\n",
    "            bpr_item_id2index_test[locale] = pickle.load(f)\n",
    "\n",
    "    # prone\n",
    "    graph_embs_test = {}\n",
    "    item_id2indices_prone_test = {}\n",
    "    for locale in LOCALES:\n",
    "        graph_embs_test[locale] = np.load(DIR + f\"models/task1/graph_embedding_03_{locale}_for_inference.npy\")\n",
    "        with open(DIR + \"data/interim/graph/task1/graph_\" + f\"item_id2index_03_{locale}_for_inference.pickle\", \"rb\") as f:\n",
    "            item_id2indices_prone_test[locale] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-NbXCWfyDZt"
   },
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D75acjHUyEyt"
   },
   "outputs": [],
   "source": [
    "# functions for data processing\n",
    "def generate_candidates(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -> pl.DataFrame:\n",
    "\n",
    "    def add_last_n_item(df: pl.DataFrame, last_n: int) -> pl.DataFrame:\n",
    "        last_item_list = []\n",
    "        prev_items_list = df[\"prev_items\"].to_list()\n",
    "        for prev_items in prev_items_list:\n",
    "            try:\n",
    "                last_item_list.append(prev_items[-last_n])\n",
    "            except IndexError:\n",
    "                last_item_list.append(None)\n",
    "        df = df.with_columns(pl.Series(name=f\"last_item_{last_n}\", values=last_item_list))\n",
    "        return df\n",
    "\n",
    "    # add last_item columns\n",
    "    for last_n in LAST_NS:\n",
    "        df = add_last_n_item(df, last_n)\n",
    "\n",
    "    # generate candidates\n",
    "    candidates = []\n",
    "\n",
    "    # candidates tied to items\n",
    "    for last_n in LAST_NS:\n",
    "        for candidate_matrix in candidate_matrices:\n",
    "            if candidate_matrix.join_key == \"item\":\n",
    "                # join candidates to last_n item\n",
    "                candidate = df.join(candidate_matrix.matrix, left_on=[f\"last_item_{last_n}\", \"locale\"], right_on=[\"item\", \"locale\"], how=\"left\")\n",
    "                candidate = candidate.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\"))) # remove already purchased items\n",
    "\n",
    "                # keep candidates for feature addition later\n",
    "                original_feat_names = candidate_matrix.feat_name\n",
    "                feat_names = [f\"{x}_last{last_n}\" for x in original_feat_names]\n",
    "                tmp = candidate[[\"session_id\", \"candidate_item\"] + original_feat_names]\n",
    "                for original_feat_name, feat_name in zip(original_feat_names, feat_names):\n",
    "                    tmp = tmp.rename({original_feat_name:feat_name})\n",
    "                candidates.append(tmp)\n",
    "\n",
    "    # candidates tied to session\n",
    "    for candidate_matrix in candidate_matrices:\n",
    "        if candidate_matrix.join_key == \"session\":\n",
    "            # join candidates to session\n",
    "            candidate = df.join(candidate_matrix.matrix, on=\"session_id\", how=\"left\")\n",
    "            candidate = candidate.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\"))) # remove already purchased items\n",
    "\n",
    "            # keep candidates for feature addition later\n",
    "            candidates.append(candidate[[\"session_id\", \"candidate_item\"] + candidate_matrix.feat_name])\n",
    "\n",
    "    cand_all = pl.concat([df[[\"session_id\", \"candidate_item\"]] for df in candidates])\n",
    "\n",
    "    # remove duplicate candidates\n",
    "    cand_all = cand_all.unique(subset=[\"session_id\", \"candidate_item\"])\n",
    "\n",
    "    # join candidates\n",
    "    df = df.join(cand_all, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "    # add features derived from the candidate\n",
    "    for candidate in candidates:\n",
    "        df = df.join(candidate, on=[\"session_id\", \"candidate_item\"], how=\"left\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_label(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.with_columns((pl.col(\"candidate_item\") == pl.col(\"next_item\")).cast(pl.Int8).alias(\"label\"))\n",
    "    return df\n",
    "\n",
    "def filter_null(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -> pl.DataFrame:\n",
    "    feat_names = []\n",
    "    for candidate_matrix in candidate_matrices:\n",
    "        if candidate_matrix.join_key == \"item\":\n",
    "            for last_n in LAST_NS:\n",
    "                for feat_name in candidate_matrix.feat_name:\n",
    "                    feat_names.append(f\"{feat_name}_last{last_n}\")\n",
    "        elif candidate_matrix.join_key == \"session\":\n",
    "            feat_names.extend(candidate_matrix.feat_name)\n",
    "    df = df.filter(\n",
    "        ~pl.all(pl.col(feat_names).is_null())\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def negative_sample(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    negatives = df.filter(df[\"label\"] == 0)\n",
    "    negatives = negatives.sample(fraction=0.1, seed=SEED)\n",
    "    df = pl.concat([df.filter(df[\"label\"] > 0), negatives])\n",
    "    return df\n",
    "\n",
    "def filter_session_not_include_positive(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    positive_sessions = df.filter(pl.col(\"label\")==1)[\"session_id\"].to_list()\n",
    "    df = df.filter(df[\"session_id\"].is_in(positive_sessions))\n",
    "    return df\n",
    "\n",
    "def add_features(\n",
    "    df: pl.DataFrame,\n",
    "    session_feat_df:pl.DataFrame, product_feat_df:pl.DataFrame,\n",
    "    i2v_models:Dict[str, Word2Vec],\n",
    "    imf_model, user_id2index, item_id2index,\n",
    "    bpr_model, bpr_user_id2index, bpr_item_id2index,\n",
    "    graph_embs, item_id2indices_prone) -> pl.DataFrame:\n",
    "\n",
    "    @njit()\n",
    "    def calc_cos_sim(v1, v2):\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    # session features\n",
    "    df = df.join(session_feat_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # product features\n",
    "    df = df.join(product_feat_df, left_on=[\"candidate_item\", \"locale\"], right_on=[\"id\", \"locale\"], how=\"left\")\n",
    "\n",
    "    # session * product features\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"P_price\") - pl.col(\"S_mean_price\")).alias(\"SP_price_diff_to_mean_price\"),\n",
    "        (pl.col(\"P_price\") - pl.col(\"S_min_price\")).alias(\"SP_price_diff_to_min_price\"),\n",
    "        (pl.col(\"P_price\") - pl.col(\"S_max_price\")).alias(\"SP_price_diff_to_max_price\"),\n",
    "        (pl.col(\"P_price\") - pl.col(\"S_last_item_price\")).alias(\"SP_price_diff_to_last_price\"),\n",
    "        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_mean_price\")).alias(\"SP_brand_price_diff_to_mean_price\"),\n",
    "        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_min_price\")).alias(\"SP_brand_price_diff_to_min_price\"),\n",
    "        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_max_price\")).alias(\"SP_brand_price_diff_to_max_price\"),\n",
    "        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_last_item_price\")).alias(\"SP_brand_price_diff_to_last_price\"),\n",
    "    ])\n",
    "\n",
    "    for last_n in LAST_NS:\n",
    "        df = df.with_columns([\n",
    "            ((pl.col(\"P_brand\") == pl.col(f\"S_brand_last{last_n}\"))&(pl.col(f\"S_brand_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_brand_last{last_n}\"),\n",
    "            ((pl.col(\"P_color\") == pl.col(f\"S_color_last{last_n}\"))&(pl.col(f\"S_color_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_color_last{last_n}\"),\n",
    "            ((pl.col(\"P_size\") == pl.col(f\"S_size_last{last_n}\"))&(pl.col(f\"S_size_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_size_last{last_n}\"),\n",
    "            ((pl.col(\"P_model\") == pl.col(f\"S_model_last{last_n}\"))&(pl.col(f\"S_model_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_model_last{last_n}\"),\n",
    "            ((pl.col(\"P_material\") == pl.col(f\"S_material_last{last_n}\"))&(pl.col(f\"S_material_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_material_last{last_n}\"),\n",
    "            ((pl.col(\"P_author\") == pl.col(f\"S_author_last{last_n}\"))&(pl.col(f\"S_author_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_author_last{last_n}\"),\n",
    "        ])\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"SP_same_brand_last1\") + pl.col(\"SP_same_brand_last2\") + pl.col(\"SP_same_brand_last3\")).cast(pl.UInt8).alias(\"SP_same_brand_sum\"),\n",
    "        (pl.col(\"SP_same_color_last1\") + pl.col(\"SP_same_color_last2\") + pl.col(\"SP_same_color_last3\")).cast(pl.UInt8).alias(\"SP_same_color_sum\"),\n",
    "        (pl.col(\"SP_same_size_last1\") + pl.col(\"SP_same_size_last2\") + pl.col(\"SP_same_size_last3\")).cast(pl.UInt8).alias(\"SP_same_size_sum\"),\n",
    "        (pl.col(\"SP_same_model_last1\") + pl.col(\"SP_same_model_last2\") + pl.col(\"SP_same_model_last3\")).cast(pl.UInt8).alias(\"SP_same_model_sum\"),\n",
    "        (pl.col(\"SP_same_material_last1\") + pl.col(\"SP_same_material_last2\") + pl.col(\"SP_same_material_last3\")).cast(pl.UInt8).alias(\"SP_same_material_sum\"),\n",
    "        (pl.col(\"SP_same_author_last1\") + pl.col(\"SP_same_author_last2\") + pl.col(\"SP_same_author_last3\")).cast(pl.UInt8).alias(\"SP_same_author_sum\"),\n",
    "    ])\n",
    "\n",
    "    # imf similarity between last items and candidates\n",
    "    dfs = []\n",
    "    for locale in list(df[\"locale\"].unique()):\n",
    "        df_by_locale = df.filter(pl.col(\"locale\") == locale)\n",
    "\n",
    "        sessions = df_by_locale[\"session_id\"].to_list()\n",
    "        candidates = df_by_locale[\"candidate_item\"].to_list()\n",
    "        user_index2vector = dict(enumerate(imf_model[locale][\"user_factors\"]))\n",
    "        item_index2vector = dict(enumerate(imf_model[locale][\"item_factors\"]))\n",
    "        imf_similarities = []\n",
    "        for session, candidate in zip(sessions, candidates):\n",
    "            try:\n",
    "                user_index, item_index = user_id2index[locale][session], item_id2index[locale][candidate]\n",
    "                v1, v2 = user_index2vector[user_index], item_index2vector[item_index]\n",
    "                sim = calc_cos_sim(v1, v2)\n",
    "            except (KeyError, TypeError): # KeyError if the item is not in the imf training data. TypeError if there are no candidates in a session.\n",
    "                sim = 0\n",
    "            imf_similarities.append(np.float32(sim))\n",
    "        df_by_locale = df_by_locale.with_columns(pl.Series(name=\"imf_similarity\", values=imf_similarities).cast(pl.Float32))\n",
    "        dfs.append(df_by_locale)\n",
    "    df = pl.concat(dfs)\n",
    "\n",
    "\n",
    "    # bpr similarity between last items and candidates\n",
    "    dfs = []\n",
    "    for locale in list(df[\"locale\"].unique()):\n",
    "        df_by_locale = df.filter(pl.col(\"locale\") == locale)\n",
    "\n",
    "        sessions = df_by_locale[\"session_id\"].to_list()\n",
    "        candidates = df_by_locale[\"candidate_item\"].to_list()\n",
    "        user_index2vector = dict(enumerate(bpr_model[locale][\"user_factors\"]))\n",
    "        item_index2vector = dict(enumerate(bpr_model[locale][\"item_factors\"]))\n",
    "        bpr_similarities = []\n",
    "        for session, candidate in zip(sessions, candidates):\n",
    "            try:\n",
    "                user_index, item_index = bpr_user_id2index[locale][session], bpr_item_id2index[locale][candidate]\n",
    "                v1, v2 = user_index2vector[user_index], item_index2vector[item_index]\n",
    "                sim = calc_cos_sim(v1, v2)\n",
    "            except (KeyError, TypeError): # KeyError if the item is not in the imf training data. TypeError if there are no candidates in a session.\n",
    "                sim = 0\n",
    "            bpr_similarities.append(np.float32(sim))\n",
    "        df_by_locale = df_by_locale.with_columns(pl.Series(name=\"bpr_similarity\", values=bpr_similarities).cast(pl.Float32))\n",
    "        dfs.append(df_by_locale)\n",
    "    df = pl.concat(dfs)\n",
    "\n",
    "    for last_n in LAST_NS:\n",
    "        # item2vec similarity between last items and candidates\n",
    "        dfs = []\n",
    "        for locale in LOCALES:\n",
    "            df_by_locale = df.filter(pl.col(\"locale\") == locale)\n",
    "\n",
    "            last_items = df_by_locale[f\"last_item_{last_n}\"].to_list()\n",
    "            cand_items = df_by_locale[\"candidate_item\"].to_list()\n",
    "            item_similalities = []\n",
    "            for last_item, cand_item in zip(last_items, cand_items):\n",
    "                try:\n",
    "                    sim = i2v_models[locale].wv.similarity(last_item, cand_item)\n",
    "                except (KeyError, TypeError): # KeyError if the item is not in the item2vec training data. TypeError if there are no candidates in a session.\n",
    "                    sim = -1\n",
    "                item_similalities.append(np.float32(sim))\n",
    "            df_by_locale = df_by_locale.with_columns(pl.Series(name=f\"i2v_similarity_last{last_n}\", values=item_similalities).cast(pl.Float32))\n",
    "            dfs.append(df_by_locale)\n",
    "        df = pl.concat(dfs)\n",
    "\n",
    "        # prone similarity between last items and candidates\n",
    "        dfs = []\n",
    "        for locale in LOCALES:\n",
    "            df_by_locale = df.filter(pl.col(\"locale\") == locale)\n",
    "            last_items = df_by_locale[f\"last_item_{last_n}\"].to_list()\n",
    "            cand_items = df_by_locale[\"candidate_item\"].to_list()\n",
    "            item_similalities = []\n",
    "            item_index2vector = dict(enumerate(graph_embs[locale]))\n",
    "            for last_item, cand_item in zip(last_items, cand_items):\n",
    "                try:\n",
    "                    item_index1 = item_id2indices_prone[locale][last_item]\n",
    "                    item_index2 = item_id2indices_prone[locale][cand_item]\n",
    "                    v1, v2 = item_index2vector[item_index1], item_index2vector[item_index2]\n",
    "                    sim = calc_cos_sim(v1, v2)\n",
    "                except (KeyError, TypeError): # KeyError if the item is not in the item2vec training data. TypeError if there are no candidates in a session.\n",
    "                    sim = -1\n",
    "                item_similalities.append(np.float32(sim))\n",
    "            df_by_locale = df_by_locale.with_columns(pl.Series(name=f\"graph_emb_similarity_last{last_n}\", values=item_similalities).cast(pl.Float32))\n",
    "            dfs.append(df_by_locale)\n",
    "        df = pl.concat(dfs)\n",
    "    return df\n",
    "\n",
    "def fill_null_and_cast(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"co_visit_weight_last1\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"consective_1_weight_last1\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"consective_3_weight_last1\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"consective_5_weight_last1\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"lift_last1\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"prone_distance_last1\").fill_null(-1).cast(pl.Float32),\n",
    "        pl.col(\"bert_distance_last1\").fill_null(-1).cast(pl.Float32),\n",
    "        pl.col(\"similarity_score_last1\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"co_visit_weight_last2\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"consective_1_weight_last2\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"consective_3_weight_last2\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"consective_5_weight_last2\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"lift_last2\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"prone_distance_last2\").fill_null(-1).cast(pl.Float32),\n",
    "        pl.col(\"bert_distance_last2\").fill_null(-1).cast(pl.Float32),\n",
    "        pl.col(\"similarity_score_last2\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"co_visit_weight_last3\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"consective_1_weight_last3\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"consective_3_weight_last3\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"consective_5_weight_last3\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"lift_last3\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"prone_distance_last3\").fill_null(-1).cast(pl.Float32),\n",
    "        pl.col(\"bert_distance_last3\").fill_null(-1).cast(pl.Float32),\n",
    "        pl.col(\"similarity_score_last3\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"imf_score\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"bpr_score\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"co_visit_rank_last1\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_1_rank_last1\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_3_rank_last1\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_5_rank_last1\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"lift_rank_last1\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"prone_rank_last1\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"bert_rank_last1\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"similarity_rank_last1\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"co_visit_rank_last2\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_1_rank_last2\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_3_rank_last2\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_5_rank_last2\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"lift_rank_last2\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"prone_rank_last2\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"bert_rank_last2\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"similarity_rank_last2\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"co_visit_rank_last3\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_1_rank_last3\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_3_rank_last3\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"consective_5_rank_last3\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"lift_rank_last3\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"prone_rank_last3\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"bert_rank_last3\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"similarity_rank_last3\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"imf_rank\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"bpr_rank\").fill_null(999).cast(pl.UInt16),\n",
    "        pl.col(\"S_locale\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"S_session_length\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"S_nunique_item\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"S_nunique_brand\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"S_color_not_null_count\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"S_size_not_null_count\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"S_model_not_null_count\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"S_material_not_null_count\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"S_author_not_null_count\").fill_null(0).cast(pl.UInt16),\n",
    "        pl.col(\"S_ratio_unique_brand\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"S_ratio_repurchase\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"S_mean_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"S_max_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"S_min_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"S_std_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"S_last_item_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"S_total_amount\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_purchase_count\").fill_null(0).cast(pl.UInt32),\n",
    "        pl.col(\"P_purchase_count_global\").fill_null(0).cast(pl.UInt32),\n",
    "        pl.col(\"P_n_unique_locale\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"P_is_color_null\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"P_is_size_null\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"P_is_model_null\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"P_is_material_null\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"P_is_author_null\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"P_brand_purchase_count\").fill_null(0).cast(pl.UInt32),\n",
    "        pl.col(\"P_brand_purchase_count_global\").fill_null(0).cast(pl.UInt32),\n",
    "        pl.col(\"P_total_amount\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_locale_purchase_count\").fill_null(0).cast(pl.UInt32),\n",
    "        pl.col(\"P_total_locale_amount\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_purchase_count_ratio_to_locale\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_purchase_amount_ratio_to_locale\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_purchase_count_ratio_to_brand\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_purchase_amount_ratio_to_brand\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_brand_mean_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_brand_max_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_brand_min_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_brand_std_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_total_brand_amount\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"P_price_diff_to_avg_brand_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_price_diff_to_mean_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_price_diff_to_min_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_price_diff_to_max_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_price_diff_to_last_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_brand_price_diff_to_mean_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_brand_price_diff_to_min_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_brand_price_diff_to_max_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_brand_price_diff_to_last_price\").fill_null(0).cast(pl.Float32),\n",
    "        pl.col(\"SP_same_brand_last1\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_brand_last2\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_brand_last3\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_color_last1\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_color_last2\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_color_last3\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_size_last1\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_size_last2\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_size_last3\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_model_last1\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_model_last2\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_model_last3\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_material_last1\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_material_last2\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_material_last3\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_author_last1\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_author_last2\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_author_last3\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_brand_sum\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_color_sum\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_size_sum\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_model_sum\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_material_sum\").fill_null(0).cast(pl.UInt8),\n",
    "        pl.col(\"SP_same_author_sum\").fill_null(0).cast(pl.UInt8),\n",
    "\n",
    "    ])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FPA8z96Zil9"
   },
   "source": [
    "## fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEckSTT1ZkDG"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d-OJvwkk8SN"
   },
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgmlzAJyRXpD"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4izMBylaa3Lq"
   },
   "outputs": [],
   "source": [
    "if MAKE_TRAIN:\n",
    "    n_rows = 50_000\n",
    "    for idx, df in tqdm(enumerate(train.iter_slices(n_rows=n_rows)), total=math.ceil(train.height/n_rows)): # specify \"total\" parameter to display tqdm progress bar\n",
    "        df = generate_candidates(df, candidate_matrices_train)\n",
    "        df = df.drop(\"prev_items\")\n",
    "        df = add_label(df)\n",
    "        df = filter_null(df, candidate_matrices_train)\n",
    "        df = filter_session_not_include_positive(df)\n",
    "        df = negative_sample(df)\n",
    "        df = add_features(df, session_feat, product_feat_train, i2v_models_train, imf_model_train, user_id2index_train, item_id2index_train, bpr_model_train, bpr_user_id2index_train, bpr_item_id2index_train, graph_embs_train, item_id2indices_prone_train)\n",
    "        df = fill_null_and_cast(df)\n",
    "        df = df[[\"session_id\", \"candidate_item\", \"label\"] + USE_FEATURES]\n",
    "        df.write_parquet(DIR + f\"data/interim/for_ranker/task1/train_chunk_{EXP_NAME}_{idx}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVIxzJGA0bXf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]<ipython-input-13-cefd9489b8d3>:102: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (Array(float64, 1, 'A', False, aligned=True), Array(float64, 1, 'A', False, aligned=True))\n",
      "  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
      "100%|██████████| 22/22 [7:19:50<00:00, 1199.57s/it]\n"
     ]
    }
   ],
   "source": [
    "if MAKE_TEST:\n",
    "    n_rows = 15_000\n",
    "    for idx, df in tqdm(enumerate(test.iter_slices(n_rows=n_rows)), total=math.ceil(test.height/n_rows)): # specify \"total\" parameter to display tqdm progress bar\n",
    "        # process data\n",
    "        df = generate_candidates(df, candidate_matrices_test)\n",
    "        df = df.drop(\"prev_items\")\n",
    "        df = add_features(df, session_feat, product_feat_test, i2v_models_test, imf_model_test, user_id2index_test, item_id2index_test, bpr_model_test, bpr_user_id2index_test, bpr_item_id2index_test, graph_embs_test, item_id2indices_prone_test)\n",
    "        df = fill_null_and_cast(df)\n",
    "        df.write_parquet(DIR + f\"data/interim/for_ranker/task1/test_chunk_{EXP_NAME}_{idx}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gibrIlkijO9Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1 (default, Dec 11 2020, 09:29:25) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7bf9227335684a71c61bfe5034b029174b8b8625eabcf049c0e38a0e70d1b74d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
